{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c02b0509",
   "metadata": {},
   "source": [
    "# Structured Output Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab7b247c",
   "metadata": {},
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e5b81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet --force-reinstall prompttools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2eac35f8",
   "metadata": {},
   "source": [
    "## Setup imports and API keys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5edba05a",
   "metadata": {},
   "source": [
    "First, we'll need to set our API keys. If we are in DEBUG mode, we don't need to use real OpenAI or Hegel AI API keys, so for now we'll set them to empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4e635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['DEBUG'] = \"1\"  # Set to \"1\" if you want to use debug mode.\n",
    "os.environ['HEGELAI_API_KEY'] = \"\"  # Optional, it will be needed to use with `HegelScribe` to persist/visualize your experiments\n",
    "os.environ['OPENAI_API_KEY'] = \"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df3934db",
   "metadata": {},
   "source": [
    "Then we'll import the relevant `prompttools` modules to setup our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0841dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "from prompttools.harness import PromptTemplateExperimentationHarness\n",
    "from prompttools.experiment import OpenAICompletionExperiment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b106a04",
   "metadata": {},
   "source": [
    "## Run experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e43545ff",
   "metadata": {},
   "source": [
    "Next, we create our test inputs. For this example, we'll use a prompt template, which uses [jinja](https://jinja.palletsprojects.com/en/3.1.x/) for templating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbab6b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_templates = [\"Generate valid JSON from the following input: {{input}}\", \"Generate valid python to complete the following task: {{input}}\"]\n",
    "user_inputs = [{\"input\": \"The task is to count all the words in a string\"}, {\"input\": \"The task is to add up numbers 1 to 100\"}]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d152fdf",
   "metadata": {},
   "source": [
    "Now we can define an experimentation harness for our inputs and model. We could also pass model arguments if, for example, we wanted to change the model temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3086e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "harness = PromptTemplateExperimentationHarness(OpenAICompletionExperiment,\n",
    "                                               \"text-davinci-003\", \n",
    "                                               prompt_templates, \n",
    "                                               user_inputs,\n",
    "                                               # Zero temperature is better for\n",
    "                                               # structured outputs\n",
    "                                               model_arguments={\"temperature\": 0})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6f5463a",
   "metadata": {},
   "source": [
    "We can then run the experiment to get results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84304957",
   "metadata": {},
   "outputs": [],
   "source": [
    "harness.run()\n",
    "harness.visualize()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b35caa9a",
   "metadata": {},
   "source": [
    "You can use the `pivot` keyword argument to view results by the template and inputs that created them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2f1bde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "harness.visualize(pivot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "266c13eb",
   "metadata": {},
   "source": [
    "## Evaluate the model response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bebb8023",
   "metadata": {},
   "source": [
    "To evaluate the results, we'll define an eval function. We can use the json and python utilities to validate our responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63de3014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompttools.utils import json\n",
    "from prompttools.utils import python\n",
    "\n",
    "def extract_responses(output) -> str:\n",
    "    rsps = [choice[\"text\"].strip() for choice in output[\"choices\"]]\n",
    "    return rsps\n",
    "\n",
    "\n",
    "def is_json(\n",
    "    input_pair: Tuple[str, Dict[str, str]], results: Dict, metadata: Dict\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    A simple test that checks semantic similarity between the user input\n",
    "    and the model's text responses.\n",
    "    \"\"\"\n",
    "    validations = [json.validate(response) for response in extract_responses(results)]\n",
    "    return min(validations)\n",
    "\n",
    "def is_python(\n",
    "    input_pair: Tuple[str, Dict[str, str]], results: Dict, metadata: Dict\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    A simple test that checks semantic similarity between the user input\n",
    "    and the model's text responses.\n",
    "    \"\"\"\n",
    "    validations = [python.validate(response) for response in extract_responses(results)]\n",
    "    return min(validations)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "974d6065",
   "metadata": {},
   "source": [
    "Finally, we can evaluate and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233f25a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "harness.evaluate(\"is_json\", is_json)\n",
    "harness.evaluate(\"is_python\", is_python)\n",
    "harness.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d282f31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
