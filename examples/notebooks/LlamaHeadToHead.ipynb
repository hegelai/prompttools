{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a13ddc8",
   "metadata": {},
   "source": [
    "# Llama vs Llama 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780dc3bf",
   "metadata": {},
   "source": [
    "Wondering how much better Llama 2 is compared to Llama?\n",
    "\n",
    "In this notebook, we'll use auto-evaluation by GPT-4 to measure outputs from both Llama and Llama 2 on a few prompts. To make this example easy to run, we'll be using 7B GGML variants of the Llama models. This should be able to run on a typical laptop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623f0cfe",
   "metadata": {},
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52881369",
   "metadata": {},
   "source": [
    "You can setup prompttools either by installing via `pip` or using `python setup.py develop` in the root of this repo. Either way, you'll need to restart the kernel after the package is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "885dabeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet --force-reinstall prompttools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eac35f8",
   "metadata": {},
   "source": [
    "## Setup imports and API keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edba05a",
   "metadata": {},
   "source": [
    "Next, we'll need to set our API keys. Since we want to use GPT-4 for auto-eval, we need to set that one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed4e635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842f1e47",
   "metadata": {},
   "source": [
    "Then we'll import the relevant `prompttools` modules to setup our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beaa70a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "from prompttools.experiment import LlamaCppExperiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622dea9a",
   "metadata": {},
   "source": [
    "## Run an experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3babfe5a",
   "metadata": {},
   "source": [
    "Next, we create our test inputs. We can iterate over models, inputs, and configurations like temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9114cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = [\n",
    "    \"/Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin\",  # Download from https://huggingface.co/TheBloke/LLaMa-7B-GGML/tree/main\n",
    "    \"/Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin\",\n",
    "]  # Download from https://huggingface.co/TheBloke/Llama-2-7B-GGML/tree/main\n",
    "prompts = [\n",
    "    \"\"\"\n",
    "    OBJECTIVE:\n",
    "    You are a sales development representative for a startup called Hegel AI.\n",
    "    Your startup builds developer tools for large language models.\n",
    "    Draft a short sales email, 50 words or less, asking a prospect for 15 minutes\n",
    "    of their time to chat about how they're using large language models.\n",
    "    \n",
    "    RESPONSE:\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    OBJECTIVE:\n",
    "    You are a customer support representative for a startup called Hegel AI.\n",
    "    Answer the following customer question:\n",
    "    Do you offer refunds?\n",
    "    \n",
    "    RESPONSE:\n",
    "    \"\"\",\n",
    "]\n",
    "temperatures = [0.0, 1.0]\n",
    "\n",
    "call_params = dict(temperature=temperatures)\n",
    "\n",
    "experiment = LlamaCppExperiment(model_paths, prompts, call_params=call_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fa5450",
   "metadata": {},
   "source": [
    "We can then run the experiment to get results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83b33130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 10 (mostly Q2_K)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 4464.12 MB (+ 1026.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "\n",
      "llama_print_timings:        load time = 12338.30 ms\n",
      "llama_print_timings:      sample time =   102.19 ms /   128 runs   (    0.80 ms per token,  1252.58 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12338.23 ms /    92 tokens (  134.11 ms per token,     7.46 tokens per second)\n",
      "llama_print_timings:        eval time = 16437.28 ms /   127 runs   (  129.43 ms per token,     7.73 tokens per second)\n",
      "llama_print_timings:       total time = 29226.18 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12338.30 ms\n",
      "llama_print_timings:      sample time =   103.25 ms /   128 runs   (    0.81 ms per token,  1239.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8712.95 ms /    42 tokens (  207.45 ms per token,     4.82 tokens per second)\n",
      "llama_print_timings:        eval time = 15836.51 ms /   127 runs   (  124.70 ms per token,     8.02 tokens per second)\n",
      "llama_print_timings:       total time = 24995.79 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12338.30 ms\n",
      "llama_print_timings:      sample time =   111.84 ms /   128 runs   (    0.87 ms per token,  1144.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9770.74 ms /    78 tokens (  125.27 ms per token,     7.98 tokens per second)\n",
      "llama_print_timings:        eval time = 15996.87 ms /   127 runs   (  125.96 ms per token,     7.94 tokens per second)\n",
      "llama_print_timings:       total time = 26234.99 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 12338.30 ms\n",
      "llama_print_timings:      sample time =   106.02 ms /   128 runs   (    0.83 ms per token,  1207.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8779.35 ms /    42 tokens (  209.03 ms per token,     4.78 tokens per second)\n",
      "llama_print_timings:        eval time = 14255.24 ms /   127 runs   (  112.25 ms per token,     8.91 tokens per second)\n",
      "llama_print_timings:       total time = 23515.30 ms\n",
      "llama.cpp: loading model from /Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 10 (mostly Q2_K)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 4525.65 MB (+ 1026.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "\n",
      "llama_print_timings:        load time = 11568.41 ms\n",
      "llama_print_timings:      sample time =   102.00 ms /   128 runs   (    0.80 ms per token,  1254.93 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11568.34 ms /    92 tokens (  125.74 ms per token,     7.95 tokens per second)\n",
      "llama_print_timings:        eval time = 15202.54 ms /   127 runs   (  119.71 ms per token,     8.35 tokens per second)\n",
      "llama_print_timings:       total time = 27217.50 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 11568.41 ms\n",
      "llama_print_timings:      sample time =   102.46 ms /   128 runs   (    0.80 ms per token,  1249.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8432.77 ms /    42 tokens (  200.78 ms per token,     4.98 tokens per second)\n",
      "llama_print_timings:        eval time = 14393.06 ms /   127 runs   (  113.33 ms per token,     8.82 tokens per second)\n",
      "llama_print_timings:       total time = 23261.17 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 11568.41 ms\n",
      "llama_print_timings:      sample time =   105.67 ms /   128 runs   (    0.83 ms per token,  1211.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =  9242.66 ms /    78 tokens (  118.50 ms per token,     8.44 tokens per second)\n",
      "llama_print_timings:        eval time = 16368.44 ms /   127 runs   (  128.89 ms per token,     7.76 tokens per second)\n",
      "llama_print_timings:       total time = 26059.39 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time = 11568.41 ms\n",
      "llama_print_timings:      sample time =   105.18 ms /   128 runs   (    0.82 ms per token,  1216.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8558.79 ms /    42 tokens (  203.78 ms per token,     4.91 tokens per second)\n",
      "llama_print_timings:        eval time = 15225.59 ms /   127 runs   (  119.89 ms per token,     8.34 tokens per second)\n",
      "llama_print_timings:       total time = 24230.96 ms\n"
     ]
    }
   ],
   "source": [
    "experiment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266c13eb",
   "metadata": {},
   "source": [
    "## Evaluate the model response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb8023",
   "metadata": {},
   "source": [
    "To evaluate the results, we'll define an eval function. We can use semantic distance to check if the model's response is similar to our expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ddbb951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompttools.utils import autoeval\n",
    "\n",
    "\n",
    "def extract_responses(output) -> str:\n",
    "    return [choice[\"text\"] for choice in output[\"choices\"]]\n",
    "\n",
    "\n",
    "def use_gpt4(row: \"pandas.core.series.Series\") -> float:\n",
    "    \"\"\"\n",
    "    A simple test that checks semantic similarity between the user input\n",
    "    and the model's text responses.\n",
    "    \"\"\"\n",
    "    prompt = row[\"prompt\"]\n",
    "    distances = [autoeval.compute(prompt, response) for response in row[\"response(s)\"]]\n",
    "    return min(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974d6065",
   "metadata": {},
   "source": [
    "Finally, we can evaluate and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e80dfeec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment.evaluate(\"auto-evaluation\", use_gpt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d09c18e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response(s)</th>\n",
       "      <th>latency</th>\n",
       "      <th>auto-evaluation</th>\n",
       "      <th>model_path</th>\n",
       "      <th>temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n    OBJECTIVE:\\n    You are a sales development representative for a startup called Hegel AI.\\n    Your startup builds developer tools for large language models.\\n    Draft a short sales email, 50 words or less, asking a prospect for 15 minutes\\n    of their time to chat about how they're using large language models.\\n    \\n    RESPONSE:\\n</td>\n",
       "      <td>[\\n    ##################################\\n    ## Hegel AI Sales Development Representative\\n    ##\\n    ## Dear [Prospect Name],\\n    ##\\n    ## I'm a sales development representative for the company Hegel AI, and we\\n    ## are building tools to help developers use large language models.\\n    ##\\n    ## We've been working on this project since 2018, and our team has grown\\n    ## from 5 people to 13 in that time.\\n    ##\\n    ## I'm reaching out because we're interested]</td>\n",
       "      <td>29.227130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n    OBJECTIVE:\\n    You are a customer support representative for a startup called Hegel AI.\\n    Answer the following customer question:\\n    Do you offer refunds?\\n    \\n    RESPONSE:\\n</td>\n",
       "      <td>[\\n        - If the user says \"yes\" then return 1, otherwise return 0\\n        - If the user says \"no\" then return 2, otherwise return 3\\n        \\n    \"\"\"\\n\\n    def __init__(self):\\n        self.num_of_questions = 1\\n        self.num_of_answers = 1\\n        self.num_of_possible_responses = 4\\n        self.num_of_possible_question_types = 2\\n        self.num_of_possible_response_types = 1\\n        self.]</td>\n",
       "      <td>24.999082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n    OBJECTIVE:\\n    You are a sales development representative for a startup called Hegel AI.\\n    Your startup builds developer tools for large language models.\\n    Draft a short sales email, 50 words or less, asking a prospect for 15 minutes\\n    of their time to chat about how they're using large language models.\\n    \\n    RESPONSE:\\n</td>\n",
       "      <td>[\\n    SPECIFICATIONS:\\n        * Use the prompt below as a starting point\\n        * Keep it under 350-words (500 words or less)\\n        \\n       A) What are your [X] keywords for natural language processing? I'm interested in them because they\\n           form part of Hegel, a free-to-use suite of developer tools for large\\n             models. I wonder if you've incorporated any of our\\n             technology into the [Y] toolkit that you built to\\n              train your core natural language processing models (ie:]</td>\n",
       "      <td>26.235267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n    OBJECTIVE:\\n    You are a customer support representative for a startup called Hegel AI.\\n    Answer the following customer question:\\n    Do you offer refunds?\\n    \\n    RESPONSE:\\n</td>\n",
       "      <td>[\\n    PARAMTERS:\\n        Question1 (String) - The full question \"Do You Offer Refunds?\"\\n        &lt;ResponseType&gt;String&lt;/ResponseType&gt;\\n            default: &lt;ResponseType string = \"No. We do not offer refunds as we are still developing the software.\"&gt;\\n\\n    SCORE SYNTAX:\\n        \\n    Q1(Question1) &lt;ResponseType String&gt;\\n\"\"\"\\nfrom unittest import TestCase, main\\nimport sys, os\\nsys.path.append('../') \\nimport json, random\\nimport H]</td>\n",
       "      <td>23.515958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n    OBJECTIVE:\\n    You are a sales development representative for a startup called Hegel AI.\\n    Your startup builds developer tools for large language models.\\n    Draft a short sales email, 50 words or less, asking a prospect for 15 minutes\\n    of their time to chat about how they're using large language models.\\n    \\n    RESPONSE:\\n</td>\n",
       "      <td>[\\n    I am a developer at Hegel AI and I have been working on a project that uses the GPT-3 model.\\n    I would like to know more about your company and how you are using the GPT-3 model in your work.\\n    Can we schedule a 15 minute call for me to learn more?\\n    \\n    DATA:\\n    - Hegel AI is a startup that builds developer tools for large language models.\\n    - The prospect works at a company called XYZ and they are using the GPT-3 model in their work.\\n   ]</td>\n",
       "      <td>27.217992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\n    OBJECTIVE:\\n    You are a customer support representative for a startup called Hegel AI.\\n    Answer the following customer question:\\n    Do you offer refunds?\\n    \\n    RESPONSE:\\n</td>\n",
       "      <td>[\\n    Yes, we do!\\n    \\n    EXPLANATION:\\n    The answer is yes because the company offers refunds.\\n    The answer is no because the company does not offer refunds.\\n\"\"\"\\n\\n# 1. Write a function that takes in a string and returns True if it contains the word \"yes\".\\n# 2. Write a function that takes in a string and returns True if it contains the word \"no\".\\n# 3. Write a function that takes in a string and returns True if it contains the word \"offer\"\\n# 4.]</td>\n",
       "      <td>23.279945</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\n    OBJECTIVE:\\n    You are a sales development representative for a startup called Hegel AI.\\n    Your startup builds developer tools for large language models.\\n    Draft a short sales email, 50 words or less, asking a prospect for 15 minutes\\n    of their time to chat about how they're using large language models.\\n    \\n    RESPONSE:\\n</td>\n",
       "      <td>[\\n    Hegel AI is an online platform that offers developer tools to help users\\n    train and test large language models like LAVaTT for speech synthesis\\n    applications, like conversational chat bots (e.g., Siri, Alexa). \\n\\n    Our platform also lets users monitor and benchmark the\\n    effectiveness of these models against standard NLP benchmarks.\\n    \\n    To book a demo, send an email to [*HegelAIDemo@hegelai.com*.](mailto:HegelAIDemo@hegelai.com)]</td>\n",
       "      <td>26.066729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>/Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\n    OBJECTIVE:\\n    You are a customer support representative for a startup called Hegel AI.\\n    Answer the following customer question:\\n    Do you offer refunds?\\n    \\n    RESPONSE:\\n</td>\n",
       "      <td>[\\n        Refunds are not available, but our free-to-use app (the “App”) may help to better understand how your child interacts with your mobile device.\\n        15. We’re excited for you to take advantage of the App as a parent in order to  see what’s happening on screen and also be aware if something \\n        happens when your child is not present (e.g., phone number, email, device name).\\n        16. Please understand that Hegel AI does not offer refunds. If your purchase has already been processed for the App]</td>\n",
       "      <td>24.233058</td>\n",
       "      <td>1.0</td>\n",
       "      <td>/Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                        prompt   \n",
       "0  \\n    OBJECTIVE:\\n    You are a sales development representative for a startup called Hegel AI.\\n    Your startup builds developer tools for large language models.\\n    Draft a short sales email, 50 words or less, asking a prospect for 15 minutes\\n    of their time to chat about how they're using large language models.\\n    \\n    RESPONSE:\\n      \\\n",
       "1  \\n    OBJECTIVE:\\n    You are a customer support representative for a startup called Hegel AI.\\n    Answer the following customer question:\\n    Do you offer refunds?\\n    \\n    RESPONSE:\\n                                                                                                                                                                 \n",
       "2  \\n    OBJECTIVE:\\n    You are a sales development representative for a startup called Hegel AI.\\n    Your startup builds developer tools for large language models.\\n    Draft a short sales email, 50 words or less, asking a prospect for 15 minutes\\n    of their time to chat about how they're using large language models.\\n    \\n    RESPONSE:\\n       \n",
       "3  \\n    OBJECTIVE:\\n    You are a customer support representative for a startup called Hegel AI.\\n    Answer the following customer question:\\n    Do you offer refunds?\\n    \\n    RESPONSE:\\n                                                                                                                                                                 \n",
       "4  \\n    OBJECTIVE:\\n    You are a sales development representative for a startup called Hegel AI.\\n    Your startup builds developer tools for large language models.\\n    Draft a short sales email, 50 words or less, asking a prospect for 15 minutes\\n    of their time to chat about how they're using large language models.\\n    \\n    RESPONSE:\\n       \n",
       "5  \\n    OBJECTIVE:\\n    You are a customer support representative for a startup called Hegel AI.\\n    Answer the following customer question:\\n    Do you offer refunds?\\n    \\n    RESPONSE:\\n                                                                                                                                                                 \n",
       "6  \\n    OBJECTIVE:\\n    You are a sales development representative for a startup called Hegel AI.\\n    Your startup builds developer tools for large language models.\\n    Draft a short sales email, 50 words or less, asking a prospect for 15 minutes\\n    of their time to chat about how they're using large language models.\\n    \\n    RESPONSE:\\n       \n",
       "7  \\n    OBJECTIVE:\\n    You are a customer support representative for a startup called Hegel AI.\\n    Answer the following customer question:\\n    Do you offer refunds?\\n    \\n    RESPONSE:\\n                                                                                                                                                                 \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        response(s)   \n",
       "0  [\\n    ##################################\\n    ## Hegel AI Sales Development Representative\\n    ##\\n    ## Dear [Prospect Name],\\n    ##\\n    ## I'm a sales development representative for the company Hegel AI, and we\\n    ## are building tools to help developers use large language models.\\n    ##\\n    ## We've been working on this project since 2018, and our team has grown\\n    ## from 5 people to 13 in that time.\\n    ##\\n    ## I'm reaching out because we're interested]                                                     \\\n",
       "1  [\\n        - If the user says \"yes\" then return 1, otherwise return 0\\n        - If the user says \"no\" then return 2, otherwise return 3\\n        \\n    \"\"\"\\n\\n    def __init__(self):\\n        self.num_of_questions = 1\\n        self.num_of_answers = 1\\n        self.num_of_possible_responses = 4\\n        self.num_of_possible_question_types = 2\\n        self.num_of_possible_response_types = 1\\n        self.]                                                                                                                           \n",
       "2  [\\n    SPECIFICATIONS:\\n        * Use the prompt below as a starting point\\n        * Keep it under 350-words (500 words or less)\\n        \\n       A) What are your [X] keywords for natural language processing? I'm interested in them because they\\n           form part of Hegel, a free-to-use suite of developer tools for large\\n             models. I wonder if you've incorporated any of our\\n             technology into the [Y] toolkit that you built to\\n              train your core natural language processing models (ie:]   \n",
       "3  [\\n    PARAMTERS:\\n        Question1 (String) - The full question \"Do You Offer Refunds?\"\\n        <ResponseType>String</ResponseType>\\n            default: <ResponseType string = \"No. We do not offer refunds as we are still developing the software.\">\\n\\n    SCORE SYNTAX:\\n        \\n    Q1(Question1) <ResponseType String>\\n\"\"\"\\nfrom unittest import TestCase, main\\nimport sys, os\\nsys.path.append('../') \\nimport json, random\\nimport H]                                                                                             \n",
       "4  [\\n    I am a developer at Hegel AI and I have been working on a project that uses the GPT-3 model.\\n    I would like to know more about your company and how you are using the GPT-3 model in your work.\\n    Can we schedule a 15 minute call for me to learn more?\\n    \\n    DATA:\\n    - Hegel AI is a startup that builds developer tools for large language models.\\n    - The prospect works at a company called XYZ and they are using the GPT-3 model in their work.\\n   ]                                                               \n",
       "5  [\\n    Yes, we do!\\n    \\n    EXPLANATION:\\n    The answer is yes because the company offers refunds.\\n    The answer is no because the company does not offer refunds.\\n\"\"\"\\n\\n# 1. Write a function that takes in a string and returns True if it contains the word \"yes\".\\n# 2. Write a function that takes in a string and returns True if it contains the word \"no\".\\n# 3. Write a function that takes in a string and returns True if it contains the word \"offer\"\\n# 4.]                                                                    \n",
       "6  [\\n    Hegel AI is an online platform that offers developer tools to help users\\n    train and test large language models like LAVaTT for speech synthesis\\n    applications, like conversational chat bots (e.g., Siri, Alexa). \\n\\n    Our platform also lets users monitor and benchmark the\\n    effectiveness of these models against standard NLP benchmarks.\\n    \\n    To book a demo, send an email to [*HegelAIDemo@hegelai.com*.](mailto:HegelAIDemo@hegelai.com)]                                                                      \n",
       "7  [\\n        Refunds are not available, but our free-to-use app (the “App”) may help to better understand how your child interacts with your mobile device.\\n        15. We’re excited for you to take advantage of the App as a parent in order to  see what’s happening on screen and also be aware if something \\n        happens when your child is not present (e.g., phone number, email, device name).\\n        16. Please understand that Hegel AI does not offer refunds. If your purchase has already been processed for the App]          \n",
       "\n",
       "     latency  auto-evaluation   \n",
       "0  29.227130  0.0              \\\n",
       "1  24.999082  0.0               \n",
       "2  26.235267  0.0               \n",
       "3  23.515958  0.0               \n",
       "4  27.217992  0.0               \n",
       "5  23.279945  0.0               \n",
       "6  26.066729  0.0               \n",
       "7  24.233058  1.0               \n",
       "\n",
       "                                                   model_path  temperature  \n",
       "0  /Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin    0.0          \n",
       "1  /Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin    0.0          \n",
       "2  /Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin    1.0          \n",
       "3  /Users/stevenkrawczyk/Downloads/llama-7b.ggmlv3.q2_K.bin    1.0          \n",
       "4  /Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin  0.0          \n",
       "5  /Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin  0.0          \n",
       "6  /Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin  1.0          \n",
       "7  /Users/stevenkrawczyk/Downloads/llama-2-7b.ggmlv3.q2_K.bin  1.0          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60a16c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
