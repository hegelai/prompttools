{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d8e180d",
   "metadata": {},
   "source": [
    "# Model Comparison Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9c3a4e",
   "metadata": {},
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4175323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet --force-reinstall prompttools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d598292",
   "metadata": {},
   "source": [
    "## Setup imports and API keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d899afc2",
   "metadata": {},
   "source": [
    "First, we'll need to set our API keys. If we are in DEBUG mode, we don't need to use real OpenAI or Hegel AI API keys, so for now we'll set them to empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "623cdd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"DEBUG\"] = \"1\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842f1e47",
   "metadata": {},
   "source": [
    "Then we'll import the relevant `prompttools` modules to setup our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dadc080f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompttools.harness import ChatModelComparisonHarness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622dea9a",
   "metadata": {},
   "source": [
    "## Run experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3babfe5a",
   "metadata": {},
   "source": [
    "Next, we create our test inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d878c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_histories = [\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273e9908",
   "metadata": {},
   "source": [
    "Now we can define an experimentation harness to compare two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f16f1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "harness = ChatModelComparisonHarness([\"gpt-3.5-turbo\", \"gpt-3.5-turbo-0613\"], chat_histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fa5450",
   "metadata": {},
   "source": [
    "We can then run the experiment to get results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "866009ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>response</th>\n",
       "      <th>latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>George Washington</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model           response   latency\n",
       "0  gpt-3.5-turbo       George Washington  0.000004\n",
       "1  gpt-3.5-turbo-0613  George Washington  0.000001"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "harness.run()\n",
    "harness.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9c5ef5",
   "metadata": {},
   "source": [
    "You can use the `pivot` keyword argument to view results by the template and inputs that created them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31776228",
   "metadata": {},
   "source": [
    "## Compare outputs manually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2ae9d9",
   "metadata": {},
   "source": [
    "We can now record our manual evaluations of the output from each model. We are evaluating the quality of the first model, based on it's side-by-side comparisons from other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "395c5710",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is disabled for now, please reach out if you would like to use this\n",
    "# harness.compare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a184e43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
