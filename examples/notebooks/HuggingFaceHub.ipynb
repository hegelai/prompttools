{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a13ddc8",
   "metadata": {},
   "source": [
    "# HuggingFace Hub Experiment Example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "623f0cfe",
   "metadata": {},
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f7dc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --force-reinstall prompttools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2eac35f8",
   "metadata": {},
   "source": [
    "## Setup imports and API keys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5edba05a",
   "metadata": {},
   "source": [
    "First, we'll need to set our API keys. If we are in DEBUG mode, we don't need to use real HuggingFaceHub or Hegel AI API keys, so for now we'll set them to empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdc47b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['DEBUG'] = \"\"  # Set to \"1\" if you want to use debug mode.\n",
    "os.environ['HEGELAI_API_KEY'] = \"\"  # Optional, it will be needed to use with `HegelScribe` to persist/visualize your experiments\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f880cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "from prompttools.experiment import HuggingFaceHubExperiment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "622dea9a",
   "metadata": {},
   "source": [
    "## Run an experiment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3babfe5a",
   "metadata": {},
   "source": [
    "Next, we create our test inputs. We can iterate over models, inputs, and configurations like temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9114cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['gpt2']\n",
    "prompts = [\n",
    "    \"Who was the first president?\",\n",
    "    \"Who was the first president of India?\",\n",
    "]\n",
    "task = ['text-generation']\n",
    "temperatures = [0.0, 1.0]\n",
    "\n",
    "experiment = HuggingFaceHubExperiment(models, prompts, task, temperature=temperatures)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3fa5450",
   "metadata": {},
   "source": [
    "We can then run the experiment to get results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b33130",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "266c13eb",
   "metadata": {},
   "source": [
    "## Evaluate the model response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bebb8023",
   "metadata": {},
   "source": [
    "To evaluate the results, we'll define an eval function. We can use semantic distance to check if the model's response is similar to our expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddbb951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompttools.utils import similarity\n",
    "\n",
    "\n",
    "EXPECTED = {\"Who was the first president?\": \"Who was the first president?\\n\\nGeorge W\",\n",
    "            \"Who was the first president of India?\": \"Rajendra Prasad\"}\n",
    "\n",
    "def extract_responses(output) -> str:\n",
    "    return [choice[\"generated_text\"] for choice in output]\n",
    "\n",
    "\n",
    "def measure_similarity(\n",
    "    prompt: str, results: Dict, metadata: Dict\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    A simple test that checks semantic similarity between the user input\n",
    "    and the model's text responses.\n",
    "    \"\"\"\n",
    "    distances = [\n",
    "        similarity.compute(EXPECTED[prompt], response)\n",
    "        for response in extract_responses(results)\n",
    "    ]\n",
    "    return min(distances)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "974d6065",
   "metadata": {},
   "source": [
    "Finally, we can evaluate and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80dfeec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment.evaluate(\"similar_to_expected\", measure_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d09c18e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2d0a45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
